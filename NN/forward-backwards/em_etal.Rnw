\documentclass{article}
\usepackage{amsmath}
\usepackage{fullpage}
\usepackage{longtable}

% Prevent floats from crossing section or subsection boundaries
\usepackage[section]{placeins}
% Credit for this voodoo goes to Heiko Oberdiek
\makeatletter
\AtBeginDocument{%
  \expandafter\renewcommand\expandafter\subsection\expandafter{%
    \expandafter\@fb@secFB\subsection
  }%
}

\makeatother
\title{forward-backward, inside-outside, viterbi, expectation maximization}
\author{Zebulun Arendsee}

\begin{document}

\maketitle

This is a family of related algorithms. The names vary between fields, but the
principles are the same. All are

\begin{itemize}
  \item fundamentally optimization problems
  \item gradient climbing functions that are guaranteed to converge to a local maximum
\end{itemize}

This is a powerful algorithm with applications to HMM, probabilistic context
free grammers (PCFG), and neural nets. Learning how the algorithm works is
often complicated by exposition coupled to complexities of one of these special
applications. I would very much like a "hello world" example that simply showed
how the algorithm works on toy data. Yet I have not found such an example.

How best to master this family depends on your background. I find the
expectation maximization algorithm most accessible.

\section{Expectation maximization (EM)}

The seminal paper on EM is \cite{dempster1977maximum}.

The wikipedia article for EM is very readable. So there isn't much point in my
rehashing it here. 

I will just point out that the point of EM in the statistical world is to
estimate maximum likelihood parameters for a model where data or variables are
missing.

For reference, here is the wikipedia introduction

\begin{verbatim}
  The EM algorithm is used to find (locally) maximum likelihood parameters of a
  statistical model in cases where the equations cannot be solved directly.
  Typically these models involve latent variables in addition to unknown
  parameters and known data observations. That is, either missing values exist
  among the data, or the model can be formulated more simply by assuming the
  existence of further unobserved data points. For example, a mixture model can
  be described more simply by assuming that each observed data point has a
  corresponding unobserved data point, or latent variable, specifying the
  mixture component to which each data point belongs.

  Finding a maximum likelihood solution typically requires taking the
  derivatives of the likelihood function with respect to all the unknown
  values, the parameters and the latent variables, and simultaneously solving
  the resulting equations. In statistical models with latent variables, this is
  usually impossible. Instead, the result is typically a set of interlocking
  equations in which the solution to the parameters requires the values of the
  latent variables and vice versa, but substituting one set of equations into
  the other produces an unsolvable equation.

  The EM algorithm proceeds from the observation that the following is a way to
  solve these two sets of equations numerically. One can simply pick arbitrary
  values for one of the two sets of unknowns, use them to estimate the second
  set, then use these new values to find a better estimate of the first set,
  and then keep alternating between the two until the resulting values both
  converge to fixed points. It's not obvious that this will work at all, but it
  can be proven that in this context it does, and that the derivative of the
  likelihood is (arbitrarily close to) zero at that point, which in turn means
  that the point is either a maximum or a saddle point.[12] In general,
  multiple maxima may occur, with no guarantee that the global maximum will be
  found. Some likelihoods also have singularities in them, i.e., nonsensical
  maxima. For example, one of the solutions that may be found by EM in a
  mixture model involves setting one of the components to have zero variance
  and the mean parameter for the same component to be equal to one of the data
  points.

  Wikipedia - Expectation Maximization Algorithm
\end{verbatim}

Here is a simple little example from phylogenetics. I've come up with this on
my own, though the idea is probably not globally original. Suppose you have a
phylogenetic tree for $n$ extant species and $m$ ancestors, and you trust the
topology of the tree, but you do not have branch lengths. Now suppose you have
a set of $k$ homologous characters (from an alphabet of size $l$) for each
species (say the amino acids in a multiple sequence alignment): $x_{ij}$ where
$i \le n$ is the species index and $j \le k$ is the character index. Each site
has a mutation rate $\mu_i$ where $i \le k$. The branch length is the
coefficient of the mutation rate, such that $\mu_i b_j$ equals the expected
number of events along branch $b_j$. We also have a transition matrix, $T$,
that specifies the chance of each change given an event has occured. We also
have the prior probabilities of each character, $\pi_i$. I will treat the
ancestral states as a superposition, where each character has a certain
probability of being the true one.

We make a few simplifying assumptions:

\begin{itemize}

  \item the topology of the tree is known

  \item the sites are independent, i.e. $x_{ia}$ is independent of
    $x_{ib}$, where $a \ne b$

  \item the mutation rates are constant

  \item there is a maximum of 1 event per branch (this is a bad assumption if $b \mu$ is high) 

\end{itemize}

Let's say we know the tree topology and the extant species characters, but we
do not know the mutation rates, ancestral states, or branch lengths.

The likelihood of a system is

\begin{align*}
  P(\boldsymbol{x}|T,\boldsymbol{b},\boldsymbol{\mu}) & =
    \left( \prod^k_1 \prod_{c=1}^{l} p_{1c} \pi_c \right) \ 
    \prod_{i,j=2}^{n+m-1}
    \prod_{k'=1}^{k}
    \left[ \left(
      \prod_{c_i=1}^{l}
      \prod_{c_j \ne c_i}^{l}
        p_{k',i,c_i} \  p_{k',j,c_j} \ \mu_{k'} b T_{{c_i}{c_j}}
    \right)
    \left(
      \prod_{c=1}^{l}
        p_{k',i,c} \  p_{k',j,c} \ (1 - \mu_{k'} b) + \mu_{k'} b T_{cc}
    \right)\right] \\
  {} & =
    \left( \prod_a^{m+n} \prod_b^k \prod_c^l \prod_d^l p_{abcd} \right)
    \left( \prod_a^{m+n-1} \prod_b^k \prod_c^{l} \prod_d^{l} \mu_b b_{a} T_{{c}{d}} \right)
    \left( \prod_a^{m+n-1} \prod_b^k \prod_c^l 1 - \mu_b b_a(1 + T_{cc}) \right)
\end{align*}

where $b$ refers the the branch length leading to the current subtree root.
The sum starts from 2, since 1 refers to root and the characters for root are
not derived.

Next I need to find the derivatives for $p$, $\mu$, $\pi$ and $b$ for the log likelihood.

\begin{equation}
  \frac{dL}{dp_{ijc}} \propto \frac{1}{p_{ijc}}
\end{equation}

\begin{equation}
  \frac{dL}{d\pi_i} \propto \frac{1}{\pi_i}
\end{equation}

\begin{align}
  \frac{dL}{d\mu_{i}} & = \frac{(m+n-1)l^2}{\mu_i} +
    \sum_a^{m+n-1} \sum_b^l  \frac{b_a(1 + T_{bb})}{1 - \mu_i b_a(1 + T_{bb})}
\end{align}

\begin{align}
  \frac{dL}{d b_{i}} & = \frac{(m+n-1)l^2}{b_i} +
    \sum_a^k \sum_b^l \frac{\mu_a(1 + T_{bb})}{1 - \mu_a b_i(1 + T_{bb})}
\end{align}

First I will make some toy data:

<<>>=
require(ape)
require(geiger)
set.seed(42)
tree <- ape::rcoal(6)
# generate characters over the leafs
transitions <- list(matrix(c(
  -1,3/5,1/5,1/5,
  3/5,-1,1/5,1/5,
  1/5,1/5,-1,3/5,
  1/5,1/5,3/5,-1
), ncol=4, byrow=TRUE))
leaf_states <- geiger::sim.char(tree, par=transitions, model='discrete', nsim=100)[,1,]
@

The first step of implementing EM is to simulate initial values:

<<>>=
initialize <- function(tree, leaf_states, k=4){

  # If there is a true edge length, remove it, since we are estimating this
  tree$edge.length <- runif(nrow(tree$edge))

  # initialize transition matrix with all equal probabilities
  transition <- matrix(rep(1/k, k^2), ncol=k)
  diag(transition) <- -1 

  size <- length(unique(c(tree$edge[,1], tree$edge[,2])))
  n_sites <- ncol(leaf_states)
  n_species <- length(tree$tip.label)
  n_ancestors <- size - n_species

  leaf_p_states <- lapply(1:n_species, FUN=function(i){
    x <- leaf_states[i, ]
    p_states <- matrix(rep(0, n_sites*k), ncol=k)
    for(j in 1:k){
      p_states[,j] <- as.numeric(x == j)
    }
    p_states
  })

  node_p_states <- lapply(1:n_ancestors, FUN=function(i){
    p_states <- matrix(runif(n_sites*k), ncol=k)
    t(apply(p_states, 1, function(x) x / sum(x)))
  })

  p_states <- append(leaf_p_states, node_p_states)

  n_nodes <- tree$Nnode

  emtree <- list(
    tree = tree,
    p = p_states,
    k = k,
    l = n_sites,
    n = n_species,
    m = n_ancestors
  )

  class(emtree) <- 'emtree'

  emtree
}
@

\section{The forward-backward (aka inside-outside) algorithm}

\section{Papers}

\subsection{Pedagogical papers}

\subsubsection{Jason Eisner (2016) {\it Inside-Outside and Forward-Backward Algorithms Are Just Backprop} \cite{eisner2016inside}}

This paper presents the algorithm mostly from the grammatical point of view.
The paper is intended to be a simple tutorial, but I think the author is a bit
out-of-touch, nonchalantly using acronyms and jargon that few outside his field
would know. The few inside his field probably don't need to read the paper. Oh
well.

\bibliographystyle{plain}
\bibliography{em_etal}

\end{document}
